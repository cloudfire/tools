<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Do not modify this file directly.  Instead, copy entries that you -->
<!-- wish to modify from this file into hdfs-site.xml and change them -->
<!-- there.  If hdfs-site.xml does not already exist, create it.      -->

<configuration>

<!-- base config  -->
<property>
  <name>dfs.data.dir</name>
  <value>/opt/hadoopdata/dfs/data</value>
  <description>Determines where on the local filesystem an DFS data node
  should store its blocks.  If this is a comma-delimited
  list of directories, then data will be stored in all named
  directories, typically on different devices.
  Directories that do not exist are ignored.
  </description>
</property>

<property>
  <name>dfs.name.dir</name>
  <value>/opt/hadoopdata/dfs/name</value>
  <description>Determines where on the local filesystem the DFS name node
      should store the name table(fsimage).  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
</property>

<property>
  <name>dfs.block.size</name>
  <value>67108864</value>
  <description>The default block size for new files.</description>
</property>

<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>



<!--   acl config  -->
<property>
  <name>dfs.web.ugi</name>
  <value>webuser,webgroup,hadoop,supergroup</value>
  <description>The user account used by the web interface.
    Syntax: USERNAME,GROUP1,GROUP2, ...
  </description>
</property>

<property>
  <name>dfs.permissions</name>
  <value>true</value>
</property>

<property>
  <name>dfs.permissions.supergroup</name>
  <value>hadoop</value>
  <description>The name of the group of super-users.</description>
</property>

<property>
  <name>dfs.https.need.client.auth</name>
  <value>false</value>
  <description>Whether SSL client certificate authentication is required
  </description>
</property>

<!--    kerberos -nameNode -config       -->

<property>
  <name>dfs.http.address</name>
    <value>c17.hadoop.index.b28.youku:50070</value>
</property>

<property>
  <name>dfs.https.address</name>
  <value>c17.hadoop.index.b28.youku:50470</value>
</property>

<property>
  <name>dfs.https.port</name>
  <value>50470</value>
</property>

<property>
  <name>dfs.block.access.token.enable</name>
  <value>true</value>
</property>

<property>
  <name>dfs.namenode.keytab.file</name>
  <value>/opt/keys/hadoop.keytab</value>
</property>

<property>
  <name>dfs.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@hdfs.server</value>
</property>

<property>
  <name>dfs.namenode.kerberos.https.principal</name>
  <value>host/_HOST@hdfs.server</value>
</property>



<!--    kerberos  secondNameNode config      -->
<property>
  <name>dfs.secondary.http.address</name>
  <value>c17.hadoop.index.b28.youku:50090</value>
</property>
<property>
  <name>dfs.secondary.https.address</name>
  <value>0.0.0.0:50495</value>
</property>
<property>
  <name>dfs.secondary.https.port</name>
  <value>50495</value>
</property>
<property>
  <name>dfs.secondary.namenode.keytab.file</name>
  <value>/opt/keys/hadoop.keytab</value> 
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.principal</name>
  <value>hadoop/_HOST@hdfs.server</value>
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.https.principal</name>
  <value>host/_HOST@hdfs.server</value>
</property>


<!--    kerberos DataNode config    -->


<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>700</value>
  <description>Permissions for the directories on on the local filesystem where 
  the DFS data node store its blocks. The permissions can either be octal or 
  symbolic.</description>
</property>
<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:1004</value>
</property>
<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:1006</value>
</property>
<property>
  <name>dfs.datanode.keytab.file</name>
  <value>/opt/keys/hadoop.keytab</value> <!-- path to the HDFS keytab -->
</property>
<property>
  <name>dfs.datanode.kerberos.principal</name>
  <value>hadoop/_HOST@hdfs.server</value>
</property>
<property>
  <name>dfs.datanode.kerberos.https.principal</name>
  <value>host/_HOST@hdfs.server</value>
</property>

<!--    other configs    -->
<property>
  <name>dfs.namenode.logging.level</name>
  <value>info</value>
  <description>The logging level for dfs namenode. Other values are "dir"(trac
e namespace mutations), "block"(trace block under/over replications and block
creations/deletions), or "all".</description>
</property>


<property>
  <name>dfs.https.server.keystore.resource</name>
  <value>ssl-server.xml</value>
  <description>Resource file from which ssl server keystore
  information will be extracted
  </description>
</property>

<property>
  <name>dfs.https.client.keystore.resource</name>
  <value>ssl-client.xml</value>
  <description>Resource file from which ssl client keystore
  information will be extracted
  </description>
</property>

<property>
  <name>dfs.safemode.threshold.pct</name>
  <value>0.999f</value>
  <description>
    Specifies the percentage of blocks that should satisfy 
    the minimal replication requirement defined by dfs.replication.min.
    Values less than or equal to 0 mean not to start in safe mode.
    Values greater than 1 will make safe mode permanent.
  </description>
</property>


<property>
  <name>dfs.support.append</name>
  <value>true</value>
  <description>Does HDFS allow appends to files?
               This is currently set to false because there are bugs in the
               "append code" and is not supported in any prodction cluster.
  </description>
</property>


</configuration>
